{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c562a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', 'and', '.', 'a', 'was', 'she', 'her', 'in', 'of', 'to', 'dorothy', 'it', 'at', 'had', 'house', 'that', 'as', 'from', 'toto', 'gray', 'aunt', 'but', 'em', 'he', 'they', 'could', 'not', 'there', 'were', 'wind', 'cyclone', 'door', 'floor', 'henry', 'him', 'one', 'uncle', 'when', 'with', 'came', 'down', 'for', 'great', 'his', 'hole', 'little', 'long', 'looked', 'now', 'room', 'small', 'so', 'up', 'air', 'all', 'around', 'bed', 'by', 'eyes', 'first', 'into', 'on', 'see', 'stood', 'sun', 'trap', 'upon', 'where', 'would', '”', 'after', 'also', 'away', 'be', 'coming', 'dark', 'fell', 'felt', 'girl', 'grass', 'if', 'made', 'miles', 'no', 'quite', 'reached', 'sat', 'side', 'sky', 'still', 'through', 'too', 'what', 'which', 'who', 'wife', '!', 'about', 'again', 'arms', 'badly', 'been', 'black', 'called', 'carried', 'caught', 'cellar', 'corner', 'did', 'dishes', 'dorothy’s', 'ears', 'easily', 'even', 'every', 'four', 'go', 'got', 'hand', 'happen', 'happened', 'hard', 'higher', 'hour', 'ladder', 'last', 'laugh', 'lay', 'like', 'lost', 'loudly', 'middle', 'never', 'north', 'nothing', 'once', 'open', 'or', 'other', 'over', 'passed', 'played', 'pressure', 'ran', 'saw', 'slowly', 'soon', 'south', 'started', 'suddenly', 'swaying', 'taken', 'their', 'then', 'three', 'until', 'very', '<eos>', 'accidents', 'across', 'afterward', 'an', 'another', 'anxiously', 'any', 'anything', 'arose', 'asleep', 'baby', 'baked', 'balloon', 'barking', 'beard', 'became', 'beds', 'before', 'being', 'beside', 'big', 'blades', 'blistered', 'boots', 'bowed', 'bring', 'broad', 'broke', 'build', 'building', 'burned', 'calmly', 'carry', 'case', 'cellar—except', 'center', 'chairs', 'changed', 'cheeks', 'child’s', 'climbed', 'close', 'closed', 'closing', 'color', 'contained', 'cookstove', 'country', 'cows', 'cracks', 'cradle', 'crawled', 'crept', 'crush', 'cupboard', 'danger', 'dashed', 'day', 'deaf', 'dearly', 'direction', 'directions', 'dog', 'doorstep', 'doorway', 'dragged', 'dropped', 'dug', 'dull', 'ear', 'edge', 'either', 'else', 'enough', 'everything', 'everywhere', 'exact', 'fall', 'family', 'far', 'farmer', 'farmer’s', 'fast', 'feather', 'few', 'find', 'flat', 'follow', 'followed', 'footing', 'found', 'fright', 'frightened', 'funny', 'future', 'garret', 'gaunt', 'generally', 'gently', 'get', 'glance', 'going', 'grayer', 'green', 'ground', 'growing', 'hair', 'halfway', 'heard', 'heart', 'here', 'hid', 'horribly', 'horses', 'hours', 'however', 'howled', 'is', 'its', 'joy', 'jumped', 'kansas', 'keeping', 'kept', 'know', 'land', 'laughed', 'laughter', 'led', 'left', 'lips', 'live', 'lived', 'lonely', 'look', 'looking', 'loved', 'low', 'lumber', 'many', 'mass', 'merrily', 'merry', 'met', 'midst', 'mighty', 'more', 'morning', 'near', 'nearly', 'night', 'nor', 'nose', 'orphan', 'out', 'paint', 'painted', 'path', 'pieces', 'playing', 'plowed', 'prairie', 'prairies', 'press', 'pretty', 'rains', 'raised', 'rarely', 'red', 'remained', 'resolved', 'riding', 'ripples', 'rocked', 'roof', 'rose', 'rough', 'running', 'rusty', 'same', 'saved', 'scream', 'screamed', 'seen', 'sharp', 'sheds', 'shook', 'shriek', 'shrieked', 'silky', 'smiled', 'sober', 'solemn', 'sparkle', 'spite', 'spoke', 'startled', 'stern', 'sticking', 'stock', 'stopped', 'storm', 'strange', 'strong', 'surroundings', 'sweep', 'table', 'terrible', 'than', 'them', 'thin', 'thing', 'this', 'those', 'thought', 'threw', 'till', 'time', 'times', 'tipped', 'today', 'told', 'top', 'tops', 'toward', 'tree', 'turned', 'twinkled', 'two', 'under', 'usual', 'voice', 'wagon', 'wail', 'wailing', 'wait', 'waited', 'walls', 'washed', 'washing', 'waves', 'way', 'wee', 'whenever', 'whirled', 'whirls', 'whirlwinds', 'whistling', 'winds', 'wonder', 'wondered', 'work', 'worked', 'worrying', 'you', 'young', '“i’ll', '“quick', '“run', '“there’s', '\\ufeff<sos>']\n",
      "<torchtext.vocab.Vocab object at 0x7b31796b7d90>\n",
      "torch.Size([1, 1279, 16])\n"
     ]
    }
   ],
   "source": [
    "# load the model encoder data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import embedding\n",
    "from embedding import d_model\n",
    "\n",
    "num_heads = 8   # number of attention heads\n",
    "d_ff = 128      # dimension of feedforward network\n",
    "num_layers = 6  # ff network layers\n",
    "\n",
    "with open(\"chapter1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "vectors, vocabulary = embedding.get_vectors(text)\n",
    "\n",
    "encoder_output = torch.load('encoded_vector.pth', weights_only=False)\n",
    "print(encoder_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "23787f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "def greedy_decode(encoder_output, tgt_embeddings, decoder, start_token_id, d_model, max_length, vocab_size):\n",
    "    final_linear = nn.Linear(d_model, vocab_size)\n",
    "    tgt_tokens = torch.full((1, 1), start_token_id)  # initialize with <SOS>\n",
    "    generated = [tgt_tokens]   # store token indices step-by-step\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        tgt_embedded = tgt_embeddings(tgt_tokens)  # [batch_size, cur_length, d_model]\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(tgt_embedded.shape[1])\n",
    "        out = decoder(tgt=tgt_embedded, memory=encoder_output, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # Project to vocab and get next token\n",
    "        logits = final_linear(out[:, -1, :])   # [batch_size, vocab_size]; use last token's output\n",
    "        next_token = torch.argmax(logits, dim=-1, keepdim=True)  # [batch_size, 1]\n",
    "        \n",
    "        print(3,next_token)\n",
    "        \n",
    "        tgt_tokens = torch.cat([tgt_tokens, next_token], dim=1)  # append new token to the input\n",
    "        generated.append(next_token)\n",
    "        \n",
    "        # You may want to stop if '<EOS>' is generated for all batch samples\n",
    "        if next_token.item() == vocabulary['<EOS>']:\n",
    "            break\n",
    "\n",
    "    return tgt_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f7f1eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z torch.Size([1, 5, 16])\n",
      "a 0\n",
      "b torch.Size([1, 1279, 16])\n",
      "3 tensor([[145]])\n",
      "3 tensor([[139]])\n",
      "3 tensor([[176]])\n",
      "3 tensor([[51]])\n",
      "3 tensor([[381]])\n",
      "3 tensor([[371]])\n",
      "3 tensor([[50]])\n",
      "3 tensor([[283]])\n",
      "3 tensor([[194]])\n",
      "3 tensor([[277]])\n",
      "3 tensor([[213]])\n",
      "3 tensor([[50]])\n",
      "3 tensor([[283]])\n",
      "3 tensor([[196]])\n",
      "3 tensor([[0]])\n",
      "tensor([[  0, 145, 139, 176,  51, 381, 371,  50, 283, 194, 277, 213,  50, 283,\n",
      "         196,   0]])\n",
      "torch.Size([1, 16])\n",
      "tensor(0)\n",
      "tensor(145)\n",
      "tensor(139)\n",
      "tensor(176)\n",
      "tensor(51)\n",
      "tensor(381)\n",
      "tensor(371)\n",
      "tensor(50)\n",
      "tensor(283)\n",
      "tensor(194)\n",
      "tensor(277)\n",
      "tensor(213)\n",
      "tensor(50)\n",
      "tensor(283)\n",
      "tensor(196)\n",
      "tensor(0)\n",
      "[0, 145, 139, 176, 51, 381, 371, 50, 283, 194, 277, 213, 50, 283, 196, 0]\n",
      "['<unk>', '<pad>', 'the', ',', 'and', '.', 'a', 'was', 'she', 'her', 'in', 'of', 'to', 'dorothy', 'it', 'at', 'had', 'house', 'that', 'as', 'from', 'toto', 'gray', 'aunt', 'but', 'em', 'he', 'they', 'could', 'not', 'there', 'were', 'wind', 'cyclone', 'door', 'floor', 'henry', 'him', 'one', 'uncle', 'when', 'with', 'came', 'down', 'for', 'great', 'his', 'hole', 'little', 'long', 'looked', 'now', 'room', 'small', 'so', 'up', 'air', 'all', 'around', 'bed', 'by', 'eyes', 'first', 'into', 'on', 'see', 'stood', 'sun', 'trap', 'upon', 'where', 'would', '”', 'after', 'also', 'away', 'be', 'coming', 'dark', 'fell', 'felt', 'girl', 'grass', 'if', 'made', 'miles', 'no', 'quite', 'reached', 'sat', 'side', 'sky', 'still', 'through', 'too', 'what', 'which', 'who', 'wife', '!', 'about', 'again', 'arms', 'badly', 'been', 'black', 'called', 'carried', 'caught', 'cellar', 'corner', 'did', 'dishes', 'dorothy’s', 'ears', 'easily', 'even', 'every', 'four', 'go', 'got', 'hand', 'happen', 'happened', 'hard', 'higher', 'hour', 'ladder', 'last', 'laugh', 'lay', 'like', 'lost', 'loudly', 'middle', 'never', 'north', 'nothing', 'once', 'open', 'or', 'other', 'over', 'passed', 'played', 'pressure', 'ran', 'saw', 'slowly', 'soon', 'south', 'started', 'suddenly', 'swaying', 'taken', 'their', 'then', 'three', 'until', 'very', '<eos>', 'accidents', 'across', 'afterward', 'an', 'another', 'anxiously', 'any', 'anything', 'arose', 'asleep', 'baby', 'baked', 'balloon', 'barking', 'beard', 'became', 'beds', 'before', 'being', 'beside', 'big', 'blades', 'blistered', 'boots', 'bowed', 'bring', 'broad', 'broke', 'build', 'building', 'burned', 'calmly', 'carry', 'case', 'cellar—except', 'center', 'chairs', 'changed', 'cheeks', 'child’s', 'climbed', 'close', 'closed', 'closing', 'color', 'contained', 'cookstove', 'country', 'cows', 'cracks', 'cradle', 'crawled', 'crept', 'crush', 'cupboard', 'danger', 'dashed', 'day', 'deaf', 'dearly', 'direction', 'directions', 'dog', 'doorstep', 'doorway', 'dragged', 'dropped', 'dug', 'dull', 'ear', 'edge', 'either', 'else', 'enough', 'everything', 'everywhere', 'exact', 'fall', 'family', 'far', 'farmer', 'farmer’s', 'fast', 'feather', 'few', 'find', 'flat', 'follow', 'followed', 'footing', 'found', 'fright', 'frightened', 'funny', 'future', 'garret', 'gaunt', 'generally', 'gently', 'get', 'glance', 'going', 'grayer', 'green', 'ground', 'growing', 'hair', 'halfway', 'heard', 'heart', 'here', 'hid', 'horribly', 'horses', 'hours', 'however', 'howled', 'is', 'its', 'joy', 'jumped', 'kansas', 'keeping', 'kept', 'know', 'land', 'laughed', 'laughter', 'led', 'left', 'lips', 'live', 'lived', 'lonely', 'look', 'looking', 'loved', 'low', 'lumber', 'many', 'mass', 'merrily', 'merry', 'met', 'midst', 'mighty', 'more', 'morning', 'near', 'nearly', 'night', 'nor', 'nose', 'orphan', 'out', 'paint', 'painted', 'path', 'pieces', 'playing', 'plowed', 'prairie', 'prairies', 'press', 'pretty', 'rains', 'raised', 'rarely', 'red', 'remained', 'resolved', 'riding', 'ripples', 'rocked', 'roof', 'rose', 'rough', 'running', 'rusty', 'same', 'saved', 'scream', 'screamed', 'seen', 'sharp', 'sheds', 'shook', 'shriek', 'shrieked', 'silky', 'smiled', 'sober', 'solemn', 'sparkle', 'spite', 'spoke', 'startled', 'stern', 'sticking', 'stock', 'stopped', 'storm', 'strange', 'strong', 'surroundings', 'sweep', 'table', 'terrible', 'than', 'them', 'thin', 'thing', 'this', 'those', 'thought', 'threw', 'till', 'time', 'times', 'tipped', 'today', 'told', 'top', 'tops', 'toward', 'tree', 'turned', 'twinkled', 'two', 'under', 'usual', 'voice', 'wagon', 'wail', 'wailing', 'wait', 'waited', 'walls', 'washed', 'washing', 'waves', 'way', 'wee', 'whenever', 'whirled', 'whirls', 'whirlwinds', 'whistling', 'winds', 'wonder', 'wondered', 'work', 'worked', 'worrying', 'you', 'young', '“i’ll', '“quick', '“run', '“there’s', '\\ufeff<sos>']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = vocabulary.__len__()\n",
    "tgt_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "\n",
    "target_string = \"<SOS> aunt em was <EOS>\"\n",
    "target_tokens = embedding.tokenize(target_string)\n",
    "token_ids = [vocabulary[token] for token in target_tokens]\n",
    "t = torch.tensor(token_ids).unsqueeze(0)\n",
    "\n",
    "emb = nn.Embedding(vocab_size, embedding_dim=d_model)\n",
    "tgt_embedding = emb(t)\n",
    "print('z',tgt_embedding.shape)\n",
    "\n",
    "\n",
    "start_token_id = vocabulary['<SOS>']\n",
    "print('a',start_token_id)\n",
    "print('b',encoder_output.shape)\n",
    "result = greedy_decode(encoder_output, tgt_embeddings, decoder, start_token_id,d_model, max_length=100, vocab_size=vocab_size)\n",
    "print(result)\n",
    "\n",
    "\n",
    "print(result.shape)\n",
    "for tokens in result[0:]:\n",
    "    for token in tokens:\n",
    "        print(token)\n",
    "\n",
    "# Suppose your decoder result is a tensor of token indices\n",
    "r = result.squeeze().tolist()\n",
    "print(r)\n",
    "\n",
    "print(vocabulary.itos)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
