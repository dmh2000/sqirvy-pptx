{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d065ae",
   "metadata": {},
   "source": [
    "project gutenberg wizard of oz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3169b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import embedding\n",
    "\n",
    "with open(\"chapter1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "vectors,vocabulary = embedding.get_vectors(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabf90b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff231b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# add positional encoding using sin/cos altorihms\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ce080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positional encoding in batch mode\n",
    "batch_size, seq_len, d_model = 64, 32, d_model\n",
    "size = vectors.shape[0]\n",
    "\n",
    "# add batch dimension for compatibility with positional encoding\n",
    "vectors = vectors.unsqueeze(0)\n",
    "\n",
    "# add positional encoding\n",
    "pos_encoder = PositionalEncoding(d_model, max_len=size)\n",
    "x  = pos_encoder(vectors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ENCODING \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # one pass of self-attention\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        # normalized feedforward network\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # execute the feedforward network  in sequence\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),  # linear transformation input\n",
    "            nn.ReLU(),                 # Rectified Linear Unit\n",
    "            nn.Dropout(dropout),       # Dropout (to prevent overfitting)\n",
    "            nn.Linear(d_ff, d_model)   # linear transformation output\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention + residual + norm\n",
    "        attn_out, _ = self.attn(x, x, x, key_padding_mask=mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        # Feedforward network + residual + norm\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "class StackedTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d678972",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8   # number of attention heads\n",
    "d_ff = 128      # dimension of feedforward network\n",
    "num_layers = 4  # number of transformer blocks\n",
    "\n",
    "mask = None   # can be a boolean mask tensor with shape [batch_size, seq_len]\n",
    "encoder = StackedTransformerEncoder(d_model, num_heads, d_ff, num_layers)\n",
    "encoded = encoder(x, mask)    \n",
    "\n",
    "# output is the updated  token vectors\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbade5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "type(encoded)\n",
    "torch.save(encoded, \"encoded_vector.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
